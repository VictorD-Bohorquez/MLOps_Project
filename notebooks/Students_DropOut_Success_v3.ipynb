{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Data Manager Class*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Data_Manager():\n",
    "    def __init__(self, path):\n",
    "        with open(path) as config:\n",
    "            self.config = yaml.safe_load(config)\n",
    "        self.data = self.load_data()\n",
    "    \n",
    "    def load_data(self):\n",
    "        path = self.config['data_load']['dataset']\n",
    "        data = pd.read_csv(path, sep=';')\n",
    "        self.data = data\n",
    "    \n",
    "    def split_data(self):\n",
    "        target = self.config['data_load']['target']\n",
    "        test = self.config['data_load']['test']\n",
    "        X = self.data.drop([target], axis=1)\n",
    "        y = self.data[target]\n",
    "        Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=test, stratify=y)\n",
    "        print(\"La dimension del conjunto de entrenamiento es: \",Xtrain.shape)\n",
    "        print(\"La dimension del conjunto de prueba es: \",Xtest.shape)\n",
    "        Xtrain.to_csv(self.config['datasets']['path_xtrain'], sep=';', index=False)\n",
    "        Xtest.to_csv(self.config['datasets']['path_xtest'], sep=';', index=False)\n",
    "        ytrain.to_csv(self.config['datasets']['path_ytrain'], sep=';', index=False)\n",
    "        ytest.to_csv(self.config['datasets']['path_ytest'], sep=';', index=False)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Feature Processor Class*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "class Feature_Manager():\n",
    "    def __init__(self, path):\n",
    "        with open(path) as config:\n",
    "            self.config = yaml.safe_load(config)\n",
    "        self.pca = PCA()\n",
    "        self.pca_components = None\n",
    "        self.log_pipe = Pipeline([('Log', FunctionTransformer(np.log1p, feature_names_out='one-to-one'))] )\n",
    "        self.log_pipe_nombres = self.config['features']['log']\n",
    "        self.scaler_pipe = Pipeline([('scaler', StandardScaler())] )\n",
    "        self.scaler_pipe_nombres = self.config['features']['scaler']\n",
    "        self.catOHE_pipeline = Pipeline( [('OneHot', OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False))] )\n",
    "        self.catOHE_pipeline_nombres = list(set(self.get_allcolumns()) - set(self.log_pipe_nombres + self.scaler_pipe_nombres))\n",
    "        self.ct_numericas = ColumnTransformer( transformers=[('log_transformer', self.log_pipe, self.log_pipe_nombres),('standard_Scaler', self.scaler_pipe, self.scaler_pipe_nombres)])\n",
    "        self.ct_categoricas = ColumnTransformer( transformers=[('cat', self.catOHE_pipeline, self.catOHE_pipeline_nombres)])\n",
    "        self.all_categories = self.get_total_categories()\n",
    "    \n",
    "    def get_allcolumns(self):\n",
    "        path = self.config['datasets']['path_xtrain']\n",
    "        data = pd.read_csv(path, sep=';')\n",
    "        return data.columns.values\n",
    "\n",
    "    def get_total_categories(self):\n",
    "        path = self.config['data_load']['dataset']\n",
    "        data = pd.read_csv(path, sep=';')\n",
    "        all_cat = self.ct_categoricas.fit_transform(data)\n",
    "        total_categories = self.ct_categoricas.named_transformers_['cat'].get_feature_names_out()\n",
    "        return total_categories\n",
    "    \n",
    "    def sync_columns(self, reference, data):\n",
    "        data = data[reference.columns]\n",
    "        return data\n",
    "    \n",
    "    def complete_features(self, data):\n",
    "        columns = data.columns\n",
    "        missig_categories = list(set(self.all_categories)-set(columns))\n",
    "        for missing in missig_categories:\n",
    "            data[missing]=0\n",
    "        return data\n",
    "\n",
    "    def get_PCA_components(self):\n",
    "        path = self.config['datasets']['path_xtrain']\n",
    "        data = pd.read_csv(path, sep=';')\n",
    "        processed = self.ct_numericas.fit_transform(data)\n",
    "        x_projected = self.pca.fit_transform(processed)\n",
    "        va = np.cumsum(self.pca.explained_variance_ratio_)\n",
    "        components = None\n",
    "        for i in range(len(va)):\n",
    "            if va[i] > 0.9:\n",
    "                print(f'El número de componentes que explican el 90% de la varianza son: {i+1}')\n",
    "                components = i+1\n",
    "                break\n",
    "        self.pca_components = components\n",
    "\n",
    "    def process_features(self, data):\n",
    "        if self.pca_components:\n",
    "            data_nums_processed = self.ct_numericas.fit_transform(data)\n",
    "            x_projected = self.pca.fit_transform(data_nums_processed)\n",
    "            x_projected = pd.DataFrame(x_projected)\n",
    "            data_cat_processed= self.ct_categoricas.fit_transform(data)\n",
    "            onehot_columns = self.ct_categoricas.named_transformers_['cat'].get_feature_names_out()\n",
    "            data_cat_processed_df = pd.DataFrame(data_cat_processed, columns=onehot_columns)\n",
    "            data_cat_processed_df = self.complete_features(data_cat_processed_df)\n",
    "            componentes = x_projected.iloc[:,0:self.pca_components]\n",
    "            componentes.reset_index(drop=True, inplace=True)\n",
    "            data_cat_processed_df.reset_index(drop=True, inplace=True)\n",
    "            data_final = pd.concat([componentes,data_cat_processed_df], axis=1)\n",
    "            data_final.columns = data_final.columns.astype(str)\n",
    "            return data_final\n",
    "        else:\n",
    "            raise Exception(\"PCA components has not been calculated, please first run the calculation to process the features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Models class*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import joblib \n",
    "\n",
    "class Models():\n",
    "    def __init__(self, path):\n",
    "        with open(path) as config:\n",
    "            self.config = yaml.safe_load(config)\n",
    "        self.models, self.names = self.generate_models()\n",
    "        self.params = self.config['tuning']['parameters']\n",
    "    \n",
    "    def generate_models(self):\n",
    "        models, names = list(), list()\n",
    "        models.append(LogisticRegression(max_iter=1000))\n",
    "        names.append('LR')\n",
    "        models.append(KNeighborsClassifier())\n",
    "        names.append('KNN')\n",
    "        models.append(DecisionTreeClassifier())\n",
    "        names.append('DTree')\n",
    "        models.append(RandomForestClassifier(n_jobs=-1))\n",
    "        names.append('RF')\n",
    "        models.append(MLPClassifier( hidden_layer_sizes=(20,20),max_iter=10000))\n",
    "        names.append('MLP')\n",
    "        models.append(SVC())\n",
    "        names.append('SVC')\n",
    "        return models, names\n",
    "    \n",
    "    def finetune_parameters(self, data, y):\n",
    "        best_parameters = self.get_hyperparameters(data, y)\n",
    "        for i in range(len(self.models)):\n",
    "            self.models[i].set_params(**best_parameters[i])\n",
    "\n",
    "    def get_hyperparameters(self, data, y):\n",
    "        parameters = []\n",
    "        for i in range(len(self.models)):\n",
    "            result = self.model_grid_search(self.models[i], self.params[self.names[i]], data, y)\n",
    "            parameters.append(result)\n",
    "        return parameters\n",
    "\n",
    "    def model_grid_search(self, model, params, X_train, y_train):\n",
    "        grid_search = GridSearchCV(estimator = model, param_grid = params, n_jobs = -1, verbose = 2, cv=3)\n",
    "        grid_search.fit(X_train, y_train.values.ravel())\n",
    "        best_mod = grid_search.best_params_\n",
    "        return  best_mod\n",
    "    \n",
    "    def validate_models(self, data, y):\n",
    "        for i in range(len(self.models)):\n",
    "            pipeline = Pipeline(steps=[('m',self.models[i])])\n",
    "            cv = RepeatedStratifiedKFold(n_splits=5,\n",
    "                                            n_repeats=15,\n",
    "                                            random_state=5\n",
    "                                            )\n",
    "            metrics = self.config['evaluation']['metrics']\n",
    "            scores = cross_validate(pipeline,\n",
    "                                    data,\n",
    "                                    np.ravel(y),\n",
    "                                    scoring=metrics,\n",
    "                                    cv=cv,\n",
    "                                    return_train_score=True,\n",
    "                                    error_score = 0,\n",
    "                                    )\n",
    "\n",
    "            print('>> %s' % self.names[i])\n",
    "            for j,k in enumerate(list(scores.keys())):\n",
    "                if j>1:\n",
    "                    print('\\t %s %.3f (%.3f)' % (k, np.mean(scores[k]),np.std(scores[k])))\n",
    "    \n",
    "    def train_models(self, Xtrain, ytrain):\n",
    "        for model in self.models:\n",
    "            model.fit(Xtrain,ytrain)\n",
    "        file = self.config['models']['path_models']\n",
    "        joblib.dump(self.models, file)\n",
    "\n",
    "    def evaluate(self, data, ytest):\n",
    "        Xtest = data\n",
    "        for i in range(len(self.models)):\n",
    "            y_pred= self.models[i].predict(Xtest)\n",
    "            print(f\"\\n>>Reporte final Test de {self.names[i]}:\")\n",
    "            print(classification_report(ytest, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Running the code*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f:\\Maestria\\MLOps\\Proyecto\\MLOps_Project\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La dimension del conjunto de entrenamiento es:  (3539, 36)\n",
      "La dimension del conjunto de prueba es:  (885, 36)\n",
      "El número de componentes que explican el 90% de la varianza son: 7\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "Fitting 3 folds for each of 210 candidates, totalling 630 fits\n",
      "Fitting 3 folds for each of 480 candidates, totalling 1440 fits\n",
      "Fitting 3 folds for each of 864 candidates, totalling 2592 fits\n",
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
      "Fitting 3 folds for each of 40 candidates, totalling 120 fits\n",
      ">> LR\n",
      "\t test_accuracy 0.759 (0.013)\n",
      "\t train_accuracy 0.795 (0.004)\n",
      "\t test_precision_macro 0.709 (0.021)\n",
      "\t train_precision_macro 0.762 (0.006)\n",
      "\t test_recall_macro 0.672 (0.017)\n",
      "\t train_recall_macro 0.715 (0.006)\n",
      "\t test_f1_macro 0.681 (0.019)\n",
      "\t train_f1_macro 0.729 (0.006)\n",
      ">> KNN\n",
      "\t test_accuracy 0.675 (0.011)\n",
      "\t train_accuracy 1.000 (0.000)\n",
      "\t test_precision_macro 0.625 (0.027)\n",
      "\t train_precision_macro 1.000 (0.000)\n",
      "\t test_recall_macro 0.538 (0.013)\n",
      "\t train_recall_macro 1.000 (0.000)\n",
      "\t test_f1_macro 0.530 (0.017)\n",
      "\t train_f1_macro 1.000 (0.000)\n",
      ">> DTree\n",
      "\t test_accuracy 0.711 (0.017)\n",
      "\t train_accuracy 0.718 (0.012)\n",
      "\t test_precision_macro 0.636 (0.067)\n",
      "\t train_precision_macro 0.645 (0.063)\n",
      "\t test_recall_macro 0.594 (0.032)\n",
      "\t train_recall_macro 0.601 (0.031)\n",
      "\t test_f1_macro 0.588 (0.049)\n",
      "\t train_f1_macro 0.596 (0.049)\n",
      ">> RF\n",
      "\t test_accuracy 0.761 (0.011)\n",
      "\t train_accuracy 1.000 (0.000)\n",
      "\t test_precision_macro 0.722 (0.019)\n",
      "\t train_precision_macro 1.000 (0.000)\n",
      "\t test_recall_macro 0.663 (0.015)\n",
      "\t train_recall_macro 1.000 (0.000)\n",
      "\t test_f1_macro 0.674 (0.017)\n",
      "\t train_f1_macro 1.000 (0.000)\n",
      ">> MLP\n",
      "\t test_accuracy 0.697 (0.024)\n",
      "\t train_accuracy 0.953 (0.030)\n",
      "\t test_precision_macro 0.642 (0.023)\n",
      "\t train_precision_macro 0.950 (0.033)\n",
      "\t test_recall_macro 0.637 (0.021)\n",
      "\t train_recall_macro 0.946 (0.034)\n",
      "\t test_f1_macro 0.637 (0.022)\n",
      "\t train_f1_macro 0.946 (0.035)\n",
      ">> SVC\n",
      "\t test_accuracy 0.766 (0.013)\n",
      "\t train_accuracy 0.803 (0.004)\n",
      "\t test_precision_macro 0.721 (0.020)\n",
      "\t train_precision_macro 0.778 (0.005)\n",
      "\t test_recall_macro 0.668 (0.017)\n",
      "\t train_recall_macro 0.715 (0.006)\n",
      "\t test_f1_macro 0.680 (0.018)\n",
      "\t train_f1_macro 0.733 (0.006)\n",
      "\n",
      ">>Reporte final Test de LR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Dropout       0.73      0.77      0.75       284\n",
      "    Enrolled       0.47      0.30      0.36       159\n",
      "    Graduate       0.79      0.86      0.82       442\n",
      "\n",
      "    accuracy                           0.73       885\n",
      "   macro avg       0.66      0.64      0.64       885\n",
      "weighted avg       0.71      0.73      0.72       885\n",
      "\n",
      "\n",
      ">>Reporte final Test de KNN:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Dropout       0.83      0.59      0.69       284\n",
      "    Enrolled       0.37      0.07      0.12       159\n",
      "    Graduate       0.65      0.95      0.77       442\n",
      "\n",
      "    accuracy                           0.68       885\n",
      "   macro avg       0.61      0.54      0.53       885\n",
      "weighted avg       0.66      0.68      0.63       885\n",
      "\n",
      "\n",
      ">>Reporte final Test de DTree:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Dropout       0.80      0.70      0.75       284\n",
      "    Enrolled       0.43      0.29      0.35       159\n",
      "    Graduate       0.75      0.90      0.82       442\n",
      "\n",
      "    accuracy                           0.73       885\n",
      "   macro avg       0.66      0.63      0.64       885\n",
      "weighted avg       0.71      0.73      0.71       885\n",
      "\n",
      "\n",
      ">>Reporte final Test de RF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Dropout       0.77      0.77      0.77       284\n",
      "    Enrolled       0.53      0.17      0.26       159\n",
      "    Graduate       0.75      0.93      0.83       442\n",
      "\n",
      "    accuracy                           0.74       885\n",
      "   macro avg       0.68      0.62      0.62       885\n",
      "weighted avg       0.72      0.74      0.71       885\n",
      "\n",
      "\n",
      ">>Reporte final Test de MLP:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Dropout       0.65      0.67      0.66       284\n",
      "    Enrolled       0.32      0.31      0.32       159\n",
      "    Graduate       0.77      0.77      0.77       442\n",
      "\n",
      "    accuracy                           0.65       885\n",
      "   macro avg       0.58      0.58      0.58       885\n",
      "weighted avg       0.65      0.65      0.65       885\n",
      "\n",
      "\n",
      ">>Reporte final Test de SVC:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Dropout       0.73      0.77      0.75       284\n",
      "    Enrolled       0.44      0.22      0.29       159\n",
      "    Graduate       0.76      0.88      0.82       442\n",
      "\n",
      "    accuracy                           0.72       885\n",
      "   macro avg       0.64      0.62      0.62       885\n",
      "weighted avg       0.69      0.72      0.70       885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data loading\n",
    "path = 'params.yaml'\n",
    "with open(path) as con:\n",
    "    config = yaml.safe_load(con)\n",
    "data_manager = Data_Manager(path)\n",
    "data_manager.load_data()\n",
    "data_manager.split_data()\n",
    "\n",
    "# Feature Engineering\n",
    "features = Feature_Manager(path)\n",
    "features.get_PCA_components()\n",
    "file = config['datasets']['path_xtrain']\n",
    "Xtrain = pd.read_csv(file, sep=';')\n",
    "Xtrain = features.process_features(Xtrain)\n",
    "Xtrain.to_csv(config['processed']['path_xtrain'], sep=';', index=False)\n",
    "file = config['datasets']['path_xtest']\n",
    "Xtest = pd.read_csv(file, sep=';')\n",
    "Xtest = features.process_features(Xtest)\n",
    "Xtest = features.sync_columns(Xtrain, Xtest)\n",
    "Xtest.to_csv(config['processed']['path_xtest'], sep=';', index=False)\n",
    "\n",
    "# Model training and validaiton\n",
    "models = Models(path)\n",
    "file = config['processed']['path_xtrain']\n",
    "Xtrain = pd.read_csv(file, sep=';')\n",
    "file = config['datasets']['path_ytrain']\n",
    "ytrain = pd.read_csv(file, sep=';')\n",
    "models.finetune_parameters(Xtrain, ytrain)\n",
    "models.validate_models(Xtrain, ytrain)\n",
    "models.train_models(Xtrain, ytrain)\n",
    "file = config['processed']['path_xtest']\n",
    "Xtest = pd.read_csv(file, sep=';')\n",
    "file = config['datasets']['path_ytest']\n",
    "ytest = pd.read_csv(file, sep=';')\n",
    "models.evaluate(Xtest, ytest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
